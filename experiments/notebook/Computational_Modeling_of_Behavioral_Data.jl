{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Modeling of Behavioral Data by Prof. Kentaro Katahira\n",
    "\n",
    "## Rescorla-Wagner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using Interact\n",
    "\n",
    "\"\"\"\n",
    "N‚Çú: number of trials\n",
    "Œ±: learning rate\n",
    "P·µ£: probability of getting reward\n",
    "\"\"\"\n",
    "\n",
    "@manipulate for N‚Çú = 0:1:500, Œ± = 0:0.05:1, P·µ£ = 0:0.05:1\n",
    "\n",
    "    ùêï = zeros(N‚Çú) #strengths of association as N‚Çú-length vector\n",
    "    ùêë = rand(N‚Çú) .< P·µ£ # presence of reinforcement (1 or 0) as N‚Çú-length vector\n",
    "\n",
    "    for t in 1: N‚Çú-1\n",
    "\n",
    "        ùêï[t+1] = ùêï[t] + Œ± *(ùêë[t]-ùêï[t])\n",
    "    end\n",
    "\n",
    "    plot(ùêï, label= string(\"a \", Œ±))\n",
    "    plot!([(i, P·µ£) for i in 1:1:N‚Çú], label=\"expected value of r: \" * string(P·µ£))\n",
    "    xlabel!(\"number of trials\")\n",
    "    ylabel!(\"strength of association\")\n",
    "    ylims!((0, 1))\n",
    "    title!(\"Rescorla-Wagner model\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning simulation\n",
    "\n",
    "\n",
    "### softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function softmax(Œ≤, Œîq)\n",
    "    return 1 / (1+ exp(-Œ≤ * (Œîq)))\n",
    "end\n",
    "\n",
    "@manipulate for Œ≤ in 0:0.05:5\n",
    "    plot([(Œîq, softmax(Œ≤, Œîq)) for Œîq in -4:0.1:4], m=:o, label=string(\"beta \", Œ≤))\n",
    "    xlabel!(\"difference in Q\")\n",
    "    ylabel!(\"probability\")\n",
    "    ylims!((0, 1))\n",
    "    title!(\"Softmax Function\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interactive plot of Q-learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "N‚Çú: number of trials\n",
    "Œ±: learning rate\n",
    "Œ≤: inverse temperature\n",
    "P·µ£: probability of getting reward in A\n",
    "\"\"\"\n",
    "\n",
    "@manipulate for N‚Çú in 0:5:200, Œ± in 0:0.05:1, Œ≤ in 0:0.25:5, P·µ£ in 0:0.05:1\n",
    "\n",
    "    ùêê = zeros((2, N‚Çú)) #initial value of Q in 2 by N‚Çú matrix\n",
    "    ùêú = zeros(Int, N‚Çú) #initial choice in each N‚Çú trial\n",
    "    ùê´ = zeros(N‚Çú) # 0 (no reward) or 1 (reward) in each N‚Çú trial\n",
    "    P‚Çê = zeros(N‚Çú) # probability of choosing A in each trial\n",
    "    P = (P·µ£, 1-P·µ£)\n",
    "\n",
    "    for t in 1:N‚Çú-1\n",
    "        P‚Çê = softmax(Œ≤, ùêê[1, t] - ùêê[2, t])\n",
    "\n",
    "        if rand() < P‚Çê\n",
    "            ùêú[t] = 1 #choose A\n",
    "            ùê´[t] = Int(rand(Float64) < P[1])\n",
    "        else\n",
    "            ùêú[t] = 2 #choose B\n",
    "            ùê´[t] = Int(rand(Float64) < P[2])\n",
    "        end\n",
    "\n",
    "        ùêê[ùêú[t], t+1] = ùêê[ùêú[t], t] + Œ± * (ùê´[t] - ùêê[ùêú[t], t])\n",
    "        ùêê[3 - ùêú[t], t+1] = ùêê[3 - ùêú[t], t] # retain value of unpicked choice\n",
    "    end\n",
    "\n",
    "    plot(ùêê[1, :], label=\"Qt(A)\", color=\"orange\")\n",
    "    plot!([(i, P[1]) for i in 1:1:N‚Çú], label=\"expected value of reward for A:\" * string(P[1]), color=\"darkorange\")\n",
    "    plot!(ùêê[2, :], label=\"Qt(B)\", color=\"skyblue\")\n",
    "    plot!([(i, P[2]) for i in 1:1:N‚Çú], label=\"expected value of reward for B:\" * string(P[2]), color=\"darkblue\")\n",
    "    xlabel!(\"number of trials\")\n",
    "    ylabel!(\"Q (value of behavior?)\")\n",
    "    ylims!((0, 1))\n",
    "    title!(\"Q-learning model\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation\n",
    "\n",
    "### Optimization with Optim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function returns a vector of choices and a vector of rewards, both of which will be used for parameter estimation\n",
    "\"\"\"\n",
    "\n",
    "function generate_qlearning_data(N‚Çú, Œ±, Œ≤, P·µ£)\n",
    "\n",
    "    ùêê = zeros((2, N‚Çú)) #initial value of Q in 2 by N‚Çú matrix\n",
    "    ùêú = zeros(Int, N‚Çú) #initial choice in each N‚Çú trial\n",
    "    ùê´ = zeros(N‚Çú) # 0 (no reward) or 1 (reward) in each N‚Çú trial\n",
    "    P‚Çê = zeros(N‚Çú) # probability of choosing A in each trial\n",
    "    P = (P·µ£, 1-P·µ£)\n",
    "\n",
    "    for t in 1:N‚Çú-1\n",
    "        P‚Çê = softmax(Œ≤, ùêê[1, t] - ùêê[2, t])\n",
    "\n",
    "        if rand() < P‚Çê\n",
    "            ùêú[t] = 1 #choose A\n",
    "            ùê´[t] = (rand(Float64) < P[1])\n",
    "        else\n",
    "            ùêú[t] = 2 #choose B\n",
    "            ùê´[t] = Int(rand(Float64) < P[2])\n",
    "        end\n",
    "\n",
    "        ùêê[ùêú[t], t+1] = ùêê[ùêú[t], t] + Œ± * (ùê´[t] - ùêê[ùêú[t], t])\n",
    "        ùêê[3 - ùêú[t], t+1] = ùêê[3 - ùêú[t], t] # retain value of unpicked choice\n",
    "    end\n",
    "\n",
    "    return ùêú, ùê´\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "init_values: [Œ±, Œ≤]\n",
    "Œ±: learning rate\n",
    "Œ≤: inverse temperature\n",
    "ùêú: vector of choices in each N‚Çú trial in 1(A) or 2(B)\n",
    "ùê´: 0 (no reward) or 1 (reward) in each N‚Çú trial\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "function func_qlearning(init_values, ùêú, ùê´) #needed for parameters to be passed as list for Optim package\n",
    "\n",
    "    N‚Çú = length(ùêú)\n",
    "    P‚Çê = zeros(N‚Çú) #probabilities of selecting A\n",
    "    ùêê = zeros((2, N‚Çú))\n",
    "    logl = 0 #initial value of log likelihood\n",
    "\n",
    "    for t in 1:N‚Çú - 1\n",
    "        P‚Çê = softmax(init_values[2], ùêê[1, t] - ùêê[2, t])\n",
    "        logl += (ùêú[t] == 1) * log(P‚Çê) + (ùêú[t] == 2) * log(1 - P‚Çê)\n",
    "        ùêê[ùêú[t], t + 1] = ùêê[ùêú[t], t] + init_values[1] * (ùê´[t] - ùêê[ùêú[t], t])\n",
    "        ùêê[3 - ùêú[t], t + 1] =  ùêê[3 - ùêú[t], t]\n",
    "    end\n",
    "\n",
    "    return (negll = -logl, ùêê = ùêê, P‚Çê = P‚Çê);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim\n",
    "\n",
    "@manipulate for N‚Çú in 0:5:200, Œ± in 0:0.05:1, Œ≤ in 0:0.25:5, P·µ£ in 0:0.05:1\n",
    "    ùêú, ùê´ = generate_qlearning_data(N‚Çú, Œ±, Œ≤, P·µ£)\n",
    "\n",
    "    func_qlearning_opt(init_values) = func_qlearning(init_values, ùêú, ùê´).negll\n",
    "\n",
    "    initial_values = rand(2)\n",
    "    lower = [0.0, 0.0]\n",
    "    upper = [1.0, 5.0]\n",
    "    inner_optimizer = GradientDescent()\n",
    "    results = optimize(func_qlearning_opt, lower, upper, initial_values, Fminbox(inner_optimizer));\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimization with BlackBoxOptim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BlackBoxOptim\n",
    "\n",
    "@manipulate for N‚Çú in 0:5:200, Œ± in 0:0.05:1, Œ≤ in 0:0.25:5, P·µ£ in 0:0.05:1\n",
    "    ùêú, ùê´ = generate_qlearning_data(N‚Çú, Œ±, Œ≤, P·µ£)\n",
    "    \n",
    "    func_qlearning_opt(init_values) = func_qlearning(init_values, ùêú, ùê´).negll\n",
    "\n",
    "    results = bboptimize(func_qlearning_opt; SearchRange = [(0.0, 1.0), (0.0, 5.0)], NumDimensions = 2);\n",
    "    best_candidate(results)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare performances when using different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_qlearning_opt(init_values) = func_qlearning([0.3, 0.4], ùêú, ùê´).negll\n",
    "compare_optimizers(func_qlearning_opt; SearchRange = [(0.0, 1.0), (0.0, 5.0)], NumDimensions = 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimization with JuMP and Ipopt packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code block generates error. How can I fix it?\n",
    "\n",
    "using JuMP, Ipopt, ForwardDiff\n",
    "\n",
    "ùêú, ùê´ = generate_qlearning_data(50, 0.6, 0.7, 0.5)\n",
    "\n",
    "func_qlearning_JuMP(Œ±, Œ≤) = func_qlearning((Œ±, Œ≤), ùêú, ùê´).negll #JuMP needs separate variables, not a list\n",
    "\n",
    "m = Model(Ipopt.Optimizer)\n",
    "register(m, :func_qlearning_JuMP, 2, func_qlearning_JuMP, autodiff=true)\n",
    "\n",
    "@variable(m, 0.0 <= x <= 1.0, start=rand())\n",
    "@variable(m, 0.0 <= y <= 5.0, start=5*rand())\n",
    "@NLobjective(m, Min, func_qlearning_JuMP(x, y))\n",
    "@show optimize!(m)\n",
    "println(\"Œ± = \", value(x), \" Œ≤ = \", value(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
